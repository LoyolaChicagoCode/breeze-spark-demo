SCRIPT_HEADER=\
"""#!/bin/bash

# Generated by cooley.py.
# Edit cooley_include.py and re-run cooley.py to generate instnaces.
#
# Parameters:
#
# nodes = %(nodes)s
# partitions = %(partitions)s
# workload = %(workload)s
# add_workload = %(add_workload_text)s
# dimension(s) of 3D array = %(dim)s^3"
#


"""

SPARK_INIT=\
"""
#
# Start Apache Spark
#

JOB_LOG=$HOME/logs/$COBALT_JOBID.txt

pushd $HOME/code/spark
cat $COBALT_NODEFILE > conf/slaves
cat $COBALT_NODEFILE >> $JOB_LOG
./sbin/start-all.sh
NODES=`wc -l conf/slaves | cut -d" " -f1`
popd

MASTER=`hostname`

echo "# Spark is now running with $NODES workers:" >> $JOB_LOG
echo "#"
echo "export SPARK_STATUS_URL=http://$MASTER.cooley.pub.alcf.anl.gov:8000" >> $JOB_LOG
echo "export SPARK_MASTER_URI=spark://$MASTER:7077" >> $JOB_LOG

SPARK_MASTER_URI=spark://$MASTER:7077
SPARK_HOME=$HOME/code/spark

#
# Done Initializing Apache Spark
#

"""


SPARK_SUBMIT=\
"""
#
# Submit Application on Spark
#

if [ -f "target/scala-2.10/demo-breeze-spark-scala-assembly-1.0.jar" ]; then
   echo "Running: "$SPARK_HOME/bin/spark-submit --master $SPARK_MASTER_URI \\
      target/scala-2.10/demo-breeze-spark-scala-assembly-1.0.jar \\
      --dim %(dim)s --nodes %(nodes)s --partitions %(partitions)s --workload %(workload)s --outputdir /home/thiruvat/logs >> $JOB_LOG
   $SPARK_HOME/bin/spark-submit --master $SPARK_MASTER_URI \\
      target/scala-2.10/demo-breeze-spark-scala-assembly-1.0.jar \\
      --dim %(dim)s --nodes %(nodes)s --partitions %(partitions)s --workload %(workload)s --outputdir /home/thiruvat/logs --json --xml >> $JOB_LOG
else
   echo "Could not find Scala target diretory. No experiments run." >> $JOB_LOG
fi

#
# Done Submitting Application on Spark
#
"""

SCRIPT_FILENAME = "%(generated)s/run-n%(nodes)s-p%(partitions)s-w%(workload)s-dim%(dim)s.sh"
